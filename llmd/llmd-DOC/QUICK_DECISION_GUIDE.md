# Quick Decision Guide: Which LLM Serving Approach?

**Date:** October 13, 2025  
**Purpose:** Help you choose the right approach for your LLM serving needs

---

## ğŸ¯ Quick Decision Tree

```
Do you need to serve LLMs in production?
â”‚
â”œâ”€ Yes â†’ Are you using RHOAI/OpenShift?
â”‚                                     â”‚
â”‚  â”œâ”€ Yes â†’ Use ServingRuntime + InferenceService (v1beta1)
â”‚                                                         â”‚
â”‚                                                         â”‚
â”‚                                                         â”‚
â”‚  â””â”€ No â†’ Do you want controller-managed or Helm-based?
â”‚                                                      â”‚
â”‚     â”œâ”€ Controller-managed â†’ âš ï¸ Wait for KServe LLMD to reach GA
â”‚                                                               â”‚
â”‚                                                               â”‚
â”‚     â””â”€ Helm-based â†’ âœ… Use llm-d Project
â”‚              ğŸ“– https://www.llm-d.ai
â”‚              ğŸ“– https://github.com/llm-d/llm-d
â”‚
â””â”€ No (Testing/Learning) â†’ Any approach works
                            Start with llm-d quickstart
```

---

## ğŸ“Š Comparison Matrix

| Feature | llm-d Project | KServe LLMD CRD | ServingRuntime + InferenceService |
|---------|---------------|-----------------|-----------------------------------|
| **Production Ready** | âœ… Yes | âŒ No (Alpha) | âœ… Yes (GA) |
| **Status** | v0.3.0 GA | v1alpha1 | v1beta1 |
| **Deployment** | Helm + helmfile | kubectl CRD | kubectl CRD |
| **Management** | Manual (Helm) | Auto (controller) | Auto (controller) |
| **Inference Scheduling** | âœ… Included (IGW) | âš ï¸ Gateway API only | âš ï¸ Basic |
| **Prefill/Decode Split** | âœ… Full support | âœ… Yes | âŒ No |
| **Multi-GPU** | âœ… Tested | âœ… Yes | âœ… Yes |
| **Multi-Accelerator** | âœ… NVIDIA, AMD, TPU, XPU | âš ï¸ Depends | âœ… Yes |
| **Benchmarks** | âœ… Published | âŒ No | âš ï¸ Limited |
| **Well-lit Paths** | âœ… 3 guides | âŒ No | âš ï¸ Basic |
| **Community Support** | âœ… Active (Slack, etc.) | âš ï¸ Limited | âœ… RHOAI support |
| **Learning Curve** | Medium | Medium | Low (RHOAI) |
| **Flexibility** | âœ… High | Medium | Low |
| **KServe Integration** | âŒ No | âœ… Native | âœ… Native |
| **RHOAI Integration** | âŒ No | âš ï¸ Limited | âœ… Full |

---

## ğŸš€ Recommendation by Use Case

### Use Case 1: Production LLM Serving (General)

**Best Choice:** llm-d Project

**Why:**
- Production-tested and benchmarked
- Intelligent inference scheduling included
- Well-documented deployment guides
- Active community support
- Multi-accelerator support

**Getting Started:**
```bash
# Clone the llm-d repo
git clone https://github.com/llm-d/llm-d.git
cd llm-d/guides
# Follow the quickstart
cat QUICKSTART.md
```

**Resources:**
- Website: https://www.llm-d.ai
- Quickstart: `/guides/QUICKSTART.md`
- Inference Scheduling: `/guides/inference-scheduling/`
- Slack: https://llm-d.ai/slack

---

### Use Case 2: Production LLM Serving in RHOAI/OpenShift

**Best Choice:** ServingRuntime + InferenceService

**Why:**
- Officially supported by Red Hat
- Integrated with RHOAI platform
- Stable (v1beta1) API
- UI integration
- Support contracts available

**Example:**
```yaml
apiVersion: serving.kserve.io/v1beta1
kind: ServingRuntime
metadata:
  name: vllm-runtime
spec:
  supportedModelFormats:
  - name: pytorch
    version: "1"
  containers:
  - name: kserve-container
    image: quay.io/opendatahub/vllm:stable
---
apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: llama-3-8b
spec:
  predictor:
    model:
      modelFormat:
        name: pytorch
      runtime: vllm-runtime
      storageUri: pvc://my-models-pvc/llama-3-8b
```

**Resources:**
- RHOAI Docs: https://access.redhat.com/documentation/en-us/red_hat_openshift_ai
- This repo: `llmd-DOC/LLMD_VS_SERVINGRUNTIME_COMPARISON_REPORT.md`

---

### Use Case 3: Experimentation / Learning

**Best Choice:** llm-d Project (simplest) or KServe LLMD (to learn CRDs)

**Why:**
- llm-d has excellent quickstart guides
- KServe LLMD is good for learning Kubernetes CRD patterns
- Both are well-documented

**Getting Started with llm-d:**
```bash
cd /home/cloud-user/temp/llm-d-repo/guides
# Follow step-by-step quickstart
less QUICKSTART.md
```

**Getting Started with KServe LLMD:**
```bash
cd /home/cloud-user/temp/kserve/llmd-test-yamls
# Review test YAMLs
ls *.yaml
```

---

### Use Case 4: Very Large Models (70B+) with Disaggregation

**Best Choice:** llm-d Project

**Why:**
- Purpose-built for prefill/decode disaggregation
- Uses NIXL for fast KV cache transfer
- Supports RDMA/InfiniBand/RoCE
- Tested on H200 clusters
- xPyD (heterogeneous P/D ratios) support

**Well-lit Paths:**
1. **Prefill/Decode Disaggregation** - `/guides/pd-disaggregation/`
   - 4 prefill workers + 1 decode worker
   - Llama-3.3-70B-Instruct-FP8
   - 8xH200 with InfiniBand

2. **Wide Expert-Parallelism** - `/guides/wide-ep-lws/`
   - For MoE models like DeepSeek-R1
   - 1 DP=8 Prefill + 2 DP=8 Decode
   - 24xH200 with InfiniBand

**Resources:**
- P/D Guide: `llm-d-repo/guides/pd-disaggregation/README.md`
- Wide EP Guide: `llm-d-repo/guides/wide-ep-lws/README.md`
- Architecture: This repo's `llmd-DOC/LLMD_PREFILL_DECODE_ARCHITECTURE.md`

---

### Use Case 5: Small Models (< 13B)

**Best Choice:** ServingRuntime + InferenceService (simplest)

**Why:**
- Don't need advanced features
- Simple deployment
- Easy to scale horizontally
- Low operational overhead

**When to consider alternatives:**
- If you need intelligent routing â†’ llm-d
- If you're already using llm-d infrastructure â†’ llm-d

---

## ğŸ” Feature-by-Feature Decision

### I need: Intelligent Inference Scheduling

**âœ… llm-d Project**
- Full inference scheduler (Endpoint Picker)
- KV-cache-aware routing
- Predicted latency balancing (experimental)
- Load-aware balancing
- Customizable scoring algorithms

**âš ï¸ KServe LLMD**
- Basic Gateway API routing
- No built-in scheduler
- Can integrate with external schedulers

**âŒ ServingRuntime + InferenceService**
- Round-robin by default
- No intelligent routing

---

### I need: Prefill/Decode Disaggregation

**âœ… llm-d Project**
- Full P/D support with NIXL
- RDMA/InfiniBand/RoCE support
- xPyD (heterogeneous ratios)
- Selective P/D
- Well-documented guide

**âœ… KServe LLMD**
- P/D support via `prefill:` spec
- Controller-managed
- âš ï¸ Alpha - may have bugs

**âŒ ServingRuntime + InferenceService**
- No P/D support

---

### I need: Multi-Accelerator Support

**âœ… llm-d Project**
- NVIDIA GPUs âœ…
- AMD GPUs (ROCm) âœ…
- Google TPUs âœ…
- Intel XPUs âœ…
- Tested and maintained

**âš ï¸ KServe LLMD**
- Depends on runtime image
- Not specifically tested for all

**âœ… ServingRuntime + InferenceService**
- NVIDIA GPUs âœ…
- AMD GPUs âœ…
- Depends on runtime

---

### I need: Production Support

**âœ… llm-d Project**
- Community support (Slack, Google Groups)
- Active development
- No commercial support
- Well-tested

**âš ï¸ KServe LLMD**
- Community support
- Alpha - no production guarantees
- Limited testing

**âœ… ServingRuntime + InferenceService**
- Red Hat support (with RHOAI subscription)
- Community support
- GA status

---

## ğŸ“š Learning Resources

### For llm-d Project:

**Official Documentation:**
- Website: https://www.llm-d.ai
- GitHub: https://github.com/llm-d/llm-d
- Quickstart: `/guides/QUICKSTART.md`
- Well-lit Paths: `/guides/README.md`

**Community:**
- Slack: https://llm-d.ai/slack
- Google Groups: https://groups.google.com/g/llm-d-contributors
- Weekly Standup: Wednesdays 12:30 PM ET

**Key Guides:**
```
/guides/inference-scheduling/  # Start here
/guides/pd-disaggregation/     # For large models
/guides/wide-ep-lws/           # For MoE models
/docs/proposals/llm-d.md       # Architecture deep dive
```

### For KServe LLMD CRD:

**This Repository:**
- Clarification: `llmd-DOC/00_IMPORTANT_CLARIFICATION.md`
- User Guide: `llmd-DOC/LLMD_USER_GUIDE_ALL_FEATURES.md`
- Technical: `llmd-DOC/LLMD_COMPLETE_ARCHITECTURE_AND_FEATURES.md`
- Testing: `llmd-DOC/LLMD_E2E_TEST_REPORT.md`

**KServe Docs:**
- GitHub: https://github.com/kserve/kserve
- Website: https://kserve.github.io/website/

### For ServingRuntime + InferenceService:

**RHOAI Documentation:**
- Official Docs: https://access.redhat.com/documentation/en-us/red_hat_openshift_ai
- This repo: `llmd-DOC/LLMD_VS_SERVINGRUNTIME_COMPARISON_REPORT.md`

**Test Examples:**
- `opendatahub-tests/tests/model_serving/model_runtime/vllm/`

---

## ğŸ“ Summary

### Quick Recommendations:

| Your Situation | Recommended Approach |
|----------------|---------------------|
| **Production + General Kubernetes** | llm-d Project |
| **Production + RHOAI/OpenShift** | ServingRuntime + InferenceService |
| **Large Models (70B+)** | llm-d Project (P/D disaggregation) |
| **MoE Models (DeepSeek-R1)** | llm-d Project (Wide EP) |
| **Small Models (< 13B)** | ServingRuntime + InferenceService |
| **Learning/Experimentation** | llm-d Project (best docs) |
| **Need Commercial Support** | RHOAI (ServingRuntime) |
| **Need Intelligent Scheduling** | llm-d Project |
| **Want Controller-Managed** | Wait for LLMD GA, or use InferenceService |
| **Want Maximum Flexibility** | llm-d Project |

### Key Insight:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                      â”‚
â”‚  For most production deployments:                    â”‚
â”‚                                                      â”‚
â”‚  â€¢ Use llm-d Project (general Kubernetes)            â”‚
â”‚  â€¢ Use ServingRuntime + InferenceService (RHOAI)     â”‚
â”‚                                                      â”‚
â”‚  KServe LLMD CRD is not yet production-ready.        â”‚
â”‚                                                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

**Need Help Deciding?**

1. Read [00_IMPORTANT_CLARIFICATION.md](./00_IMPORTANT_CLARIFICATION.md)
2. Check [LLMD_VS_SERVINGRUNTIME_COMPARISON_REPORT.md](./LLMD_VS_SERVINGRUNTIME_COMPARISON_REPORT.md)
3. Try llm-d quickstart: `llm-d-repo/guides/QUICKSTART.md`

**Still unsure? Start with llm-d Project** - it's production-ready, well-documented, and provides the most flexibility.


