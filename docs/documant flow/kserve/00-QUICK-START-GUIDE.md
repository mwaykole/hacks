# KServe Quick Start Guide

## Overview

This is a quick reference guide to help you navigate the KServe documentation. All diagrams use **Mermaid flowcharts** for visualization.

## ğŸš€ 5-Minute Understanding

### What is KServe?

KServe is a **Kubernetes-native platform** for serving both **Generative AI (LLMs)** and **Predictive AI (traditional ML)** models at scale.

```mermaid
flowchart LR
    You[You] -->|Deploy Model| KServe[KServe]
    KServe -->|Creates| Pod[Inference Pod]
    Pod -->|Serves| Model[Your ML Model]
    Client[Clients] -->|Request| Model
    Model -->|Prediction| Client
    
    style KServe fill:#e1f5ff
    style Pod fill:#fff4e1
    style Model fill:#99ff99
```

### Core Concepts

```mermaid
flowchart TB
    subgraph Concepts["Core Concepts"]
        ISVC[InferenceService<br/>Main resource you create]
        Predictor[Predictor<br/>Serves your model]
        Runtime[ServingRuntime<br/>How to run model]
        Storage[Storage<br/>Where model is stored]
    end
    
    You[You] -->|1. Create| ISVC
    ISVC -->|2. Specifies| Runtime
    ISVC -->|3. Loads from| Storage
    ISVC -->|4. Deploys| Predictor
    
    style ISVC fill:#e1f5ff
    style Predictor fill:#fff4e1
    style Runtime fill:#f0e1ff
    style Storage fill:#e1ffe1
```

## ğŸ“– Documentation Roadmap

### By Experience Level

#### **Beginner** (Start Here)
```mermaid
flowchart LR
    Start[Start] -->|1| Overview[Overall Architecture<br/>ğŸ“„ 01]
    Overview -->|2| DataPlane[Data Plane<br/>ğŸ“„ 03]
    DataPlane -->|3| Storage[Storage Initializer<br/>ğŸ“„ 04]
    Storage -->|4| Predictor[Predictor Runtime<br/>ğŸ“„ 05]
    
    style Overview fill:#99ff99
    style DataPlane fill:#99ff99
    style Storage fill:#99ff99
    style Predictor fill:#99ff99
```

**Start with**: 
1. [Overall Architecture](./01-KSERVE-OVERALL-ARCHITECTURE.md) - Big picture
2. [Data Plane Components](./03-DATA-PLANE-COMPONENTS.md) - How it works
3. [Storage Initializer](./04-STORAGE-INITIALIZER.md) - Model loading
4. [Predictor Runtime](./05-PREDICTOR-RUNTIME.md) - Model serving

#### **Intermediate** (Deep Dive)
```mermaid
flowchart TB
    Controller[InferenceService Controller<br/>ğŸ“„ 02] 
    Transformer[Transformer Component<br/>ğŸ“„ 06]
    Explainer[Explainer Component<br/>ğŸ“„ 07]
    Graph[InferenceGraph<br/>ğŸ“„ 08]
    
    Controller --> Transformer
    Transformer --> Explainer
    Explainer --> Graph
    
    style Controller fill:#99ccff
    style Transformer fill:#99ccff
    style Explainer fill:#99ccff
    style Graph fill:#99ccff
```

**Continue with**:
- [InferenceService Controller](./02-INFERENCESERVICE-CONTROLLER.md) - Control plane

#### **Advanced** (Expert Level)
```mermaid
flowchart LR
    ModelMesh[ModelMesh<br/>ğŸ“„ 09]
    Knative[Knative<br/>ğŸ“„ 10]
    Autoscale[Autoscaling<br/>ğŸ“„ 11]
    Protocols[Protocols<br/>ğŸ“„ 12]
    
    ModelMesh --> Knative
    Knative --> Autoscale
    Autoscale --> Protocols
    
    style ModelMesh fill:#ff99cc
    style Knative fill:#ff99cc
    style Autoscale fill:#ff99cc
    style Protocols fill:#ff99cc
```

### By Use Case

#### **Deploying LLMs / Generative AI**

```mermaid
flowchart TB
    Start[Want to Deploy LLM]
    
    Start --> Arch[ğŸ“„ 01: Overall Architecture<br/>Section: Generative AI Features]
    Arch --> Predictor[ğŸ“„ 05: Predictor Runtime<br/>Section: LLM-Specific Features]
    Predictor --> Storage[ğŸ“„ 04: Storage Initializer<br/>Load Large Models]
    Storage --> Deploy[Ready to Deploy!]
    
    style Start fill:#e1f5ff
    style Deploy fill:#99ff99
```

**Key Topics**:
- vLLM Runtime
- GPU Memory Management
- KV Cache Offloading
- OpenAI Protocol
- Streaming Responses

#### **Deploying Traditional ML Models**

```mermaid
flowchart TB
    Start[Want to Deploy ML Model]
    
    Start --> Arch[ğŸ“„ 01: Overall Architecture<br/>Section: Predictive AI Features]
    Arch --> DataPlane[ğŸ“„ 03: Data Plane Components<br/>Component Overview]
    DataPlane --> Predictor[ğŸ“„ 05: Predictor Runtime<br/>Select Framework]
    Predictor --> Deploy[Ready to Deploy!]
    
    style Start fill:#fff4e1
    style Deploy fill:#99ff99
```

**Key Topics**:
- TensorFlow/PyTorch/SKLearn
- Transformer (Pre/Post-processing)
- Explainer (Model Interpretability)
- Batching

#### **High-Scale Multi-Model Serving**

```mermaid
flowchart TB
    Start[Need High-Density Serving]
    
    Start --> Arch[ğŸ“„ 01: Overall Architecture<br/>Section: ModelMesh Mode]
    Arch --> Deploy[Ready for ModelMesh!]
    
    style Start fill:#f0e1ff
    style Deploy fill:#99ff99
```

#### **Auto-Scaling Setup**

```mermaid
flowchart TB
    Start[Need Auto-Scaling]
    
    Start --> Mode{Deployment Mode?}
    Mode -->|Serverless| Knative[ğŸ“„ 10: Knative Integration]
    Mode -->|Raw K8s| HPA[ğŸ“„ 11: Autoscaling<br/>Section: HPA]
    
    style Start fill:#e1ffe1
```

## ğŸ“‹ Component Quick Reference

```mermaid
flowchart TB
    subgraph ControlPlane["Control Plane"]
        Controller[InferenceService Controller<br/>Manages lifecycle<br/>Doc 02]
    end
    
    subgraph DataPlane["Data Plane"]
        Storage[Storage Initializer<br/>Loads models<br/>Doc 04]
        Predictor[Predictor<br/>Serves model<br/>Doc 05]
        Optional[Transformer + Explainer<br/>Pre/Post-process<br/>Doc 06, 07]
    end
    
    subgraph Integration["Integration"]
        Knative[Knative<br/>Serverless<br/>Doc 10]
        ModelMesh[ModelMesh<br/>Multi-model<br/>Doc 09]
    end
    
    ControlPlane -->|Orchestrates| DataPlane
    ControlPlane -->|Uses| Integration
    
    style ControlPlane fill:#e1f5ff
    style DataPlane fill:#fff4e1
    style Integration fill:#f0e1ff
```

## ğŸ¯ Common Tasks

### Task 1: Deploy Your First Model

```mermaid
flowchart LR
    Read1[Read:<br/>Overall Architecture] -->
    Read2[Read:<br/>Predictor Runtime] -->
    Read3[Read:<br/>Storage Initializer] -->
    Deploy[Deploy!]
    
    style Deploy fill:#99ff99
```

**Documentation Path**:
1. [Overall Architecture](./01-KSERVE-OVERALL-ARCHITECTURE.md) â†’ Deployment Modes
2. [Predictor Runtime](./05-PREDICTOR-RUNTIME.md) â†’ Select your framework
3. [Storage Initializer](./04-STORAGE-INITIALIZER.md) â†’ Configure storage

### Task 2: Add Pre-processing

```mermaid
flowchart LR
    Have[Have Basic Deployment] -->
    Read[Read:<br/>Transformer Component] -->
    Add[Add Transformer]
    
    style Add fill:#99ff99
```

**Documentation Path**:
- Coming: [Transformer Component](./06-TRANSFORMER-COMPONENT.md)

### Task 3: Enable Auto-Scaling

```mermaid
flowchart LR
    Have[Have Deployment] -->
    Check{Serverless?}
    Check -->|Yes| Knative[Read: Knative]
    Check -->|No| HPA[Read: Autoscaling]
    Knative --> Configure[Configure]
    HPA --> Configure
    
    style Configure fill:#99ff99
```

**Documentation Path**:
- Coming: [Knative Integration](./10-KNATIVE-INTEGRATION.md)
- Coming: [Autoscaling Mechanisms](./11-AUTOSCALING-MECHANISMS.md)

### Task 4: Setup Canary Deployment

```mermaid
flowchart LR
    Have[Have Deployment] -->
    Read[Read:<br/>InferenceService Controller<br/>Traffic Management] -->
    Deploy[Deploy Canary]
    
    style Deploy fill:#99ff99
```

**Documentation Path**:
- [InferenceService Controller](./02-INFERENCESERVICE-CONTROLLER.md) â†’ Traffic Management

## ğŸ” Find Information About...

### Components

| Component | Documentation | Key Topics |
|-----------|--------------|------------|
| **InferenceService** | [ğŸ“„ 02](./02-INFERENCESERVICE-CONTROLLER.md) | CRD, Reconciliation, Webhooks |
| **Storage** | [ğŸ“„ 04](./04-STORAGE-INITIALIZER.md) | S3, GCS, Azure, PVC, HTTP |
| **Predictor** | [ğŸ“„ 05](./05-PREDICTOR-RUNTIME.md) | Runtimes, GPU, LLMs |
| **Transformer** | ğŸ”œ 06 | Pre/Post-processing |
| **Explainer** | ğŸ”œ 07 | Model interpretability |
| **Router** | ğŸ”œ 08 | InferenceGraph, Pipelines |

### Features

| Feature | Where to Find |
|---------|---------------|
| **LLM Serving** | [ğŸ“„ 05](./05-PREDICTOR-RUNTIME.md) â†’ LLM Features |
| **GPU Support** | [ğŸ“„ 05](./05-PREDICTOR-RUNTIME.md) â†’ GPU Management |
| **Batching** | [ğŸ“„ 05](./05-PREDICTOR-RUNTIME.md) â†’ Dynamic Batching |
| **Scale-to-Zero** | [ğŸ“„ 01](./01-KSERVE-OVERALL-ARCHITECTURE.md) + ğŸ”œ 10 |
| **Canary Rollout** | [ğŸ“„ 02](./02-INFERENCESERVICE-CONTROLLER.md) â†’ Traffic |
| **Model Caching** | [ğŸ“„ 04](./04-STORAGE-INITIALIZER.md) + [ğŸ“„ 05](./05-PREDICTOR-RUNTIME.md) |

### Deployment Modes

```mermaid
flowchart TB
    Question[Which Mode?]
    
    Question -->|Want Scale-to-Zero| Serverless[Serverless Mode<br/>ğŸ“„ 01, 10]
    Question -->|Want Simplicity| Raw[Raw K8s Mode<br/>ğŸ“„ 01]
    Question -->|Want Multi-Model| MM[ModelMesh Mode<br/>ğŸ“„ 01, 09]
    
    style Serverless fill:#e1f5ff
    style Raw fill:#fff4e1
    style MM fill:#f0e1ff
```

## ğŸ“Š Documentation Status

| Document | Status | Topics Covered |
|----------|--------|----------------|
| ğŸ“„ 00 Quick Start | âœ… Complete | Navigation guide |
| ğŸ“„ 01 Overall Architecture | âœ… Complete | Full architecture |
| ğŸ“„ 02 InferenceService Controller | âœ… Complete | Control plane |
| ğŸ“„ 03 Data Plane Components | âœ… Complete | Runtime components |
| ğŸ“„ 04 Storage Initializer | âœ… Complete | Model loading |
| ğŸ“„ 05 Predictor Runtime | âœ… Complete | Model serving |
| ğŸ“„ 06 Transformer | ğŸ”œ Coming | Pre/Post-processing |
| ğŸ“„ 07 Explainer | ğŸ”œ Coming | Interpretability |
| ğŸ“„ 08 InferenceGraph | ğŸ”œ Coming | Routing, Pipelines |
| ğŸ“„ 09 ModelMesh | ğŸ”œ Coming | Multi-model serving |
| ğŸ“„ 10 Knative | ğŸ”œ Coming | Serverless |
| ğŸ“„ 11 Autoscaling | ğŸ”œ Coming | Scaling mechanisms |
| ğŸ“„ 12 Protocols | ğŸ”œ Coming | V1, V2, OpenAI |

## ğŸ“ Learning Paths

### Path 1: Quick Deployment (30 minutes)
```
Overall Architecture (10m) 
    â†“
Predictor Runtime (10m)
    â†“
Storage Initializer (10m)
    â†“
Deploy your first model!
```

### Path 2: Full Understanding (2 hours)
```
Overall Architecture (15m)
    â†“
InferenceService Controller (30m)
    â†“
Data Plane Components (30m)
    â†“
Storage Initializer (20m)
    â†“
Predictor Runtime (25m)
    â†“
Expert level!
```

### Path 3: LLM Specialist (1 hour)
```
Overall Architecture â†’ Generative AI (15m)
    â†“
Predictor Runtime â†’ LLM Features (30m)
    â†“
Storage Initializer â†’ Large Models (15m)
    â†“
Deploy LLMs!
```

## ğŸš¦ Getting Started Checklist

- [ ] Read [Overall Architecture](./01-KSERVE-OVERALL-ARCHITECTURE.md)
- [ ] Understand deployment modes
- [ ] Choose your serving runtime
- [ ] Configure storage for your model
- [ ] Review [Predictor Runtime](./05-PREDICTOR-RUNTIME.md) for your framework
- [ ] Deploy your first InferenceService
- [ ] Test inference endpoint
- [ ] Set up monitoring
- [ ] Configure autoscaling
- [ ] Deploy to production!

## ğŸ”— External Resources

### Official Documentation
- [KServe Website](https://kserve.github.io/website/)
- [KServe GitHub](https://github.com/kserve/kserve)
- [Getting Started Guide](https://kserve.github.io/website/docs/getting-started)

### Related Projects
- [OpenDataHub KServe](https://github.com/opendatahub-io/kserve)
- [Knative Serving](https://knative.dev/docs/serving/)
- [ModelMesh](https://github.com/kserve/modelmesh-serving)

## ğŸ’¡ Tips for Using This Documentation

1. **Start with the flowcharts**: Visual understanding first
2. **Follow the links**: Documentation is interconnected
3. **Use the search guide**: In README.md
4. **Check "Related Components"**: At the end of each doc
5. **Refer to examples**: YAML specs provided throughout

## â“ FAQ Navigation

**Q: How do I deploy an LLM?**
â†’ [Overall Architecture](./01-KSERVE-OVERALL-ARCHITECTURE.md) â†’ Generative AI
â†’ [Predictor Runtime](./05-PREDICTOR-RUNTIME.md) â†’ LLM Features

**Q: How does autoscaling work?**
â†’ [Overall Architecture](./01-KSERVE-OVERALL-ARCHITECTURE.md) â†’ Autoscaling
â†’ Coming: Autoscaling Mechanisms

**Q: How do I load models from S3?**
â†’ [Storage Initializer](./04-STORAGE-INITIALIZER.md) â†’ S3 Download Flow

**Q: What's the difference between deployment modes?**
â†’ [Overall Architecture](./01-KSERVE-OVERALL-ARCHITECTURE.md) â†’ Deployment Modes

**Q: How do I add pre-processing?**
â†’ Coming: [Transformer Component](./06-TRANSFORMER-COMPONENT.md)

---

**Ready to dive in?** Start with the [Overall Architecture](./01-KSERVE-OVERALL-ARCHITECTURE.md)!

